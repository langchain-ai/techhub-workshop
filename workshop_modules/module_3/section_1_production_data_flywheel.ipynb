{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 3, Section 1: Deployment & Continuous Improvement\n",
        "\n",
        "We've built a robust customer support agent and validated it works well on our test dataset. Now it's time to **deploy it to production** and set up continuous monitoring.\n",
        "\n",
        "In this section, we'll learn how to:\n",
        "1. Deploy our agent to LangSmith Deployments\n",
        "2. Set up **online evaluation** to monitor production performance\n",
        "3. Create an **annotation queue** for human review of flagged traces\n",
        "4. Build an **automation rule** to populate the queue with failures\n",
        "5. Walk through a complete example of the data flywheel in action\n",
        "\n",
        "**The Data Flywheel:**\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=\"../../images/data_flywheel.png\" width=\"700\">\n",
        "</div>\n",
        "\n",
        "Online eval ‚Üí Automation rule ‚Üí Annotation queue ‚Üí Golden dataset ‚Üí Offline eval ‚Üí Improved agent ‚Üí Repeat!\n",
        "\n",
        "This creates a **continuous improvement loop** where production failures automatically feed back into your development process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--- \n",
        "\n",
        "## 1. Creating a Deployment\n",
        "\n",
        "A **deployment** is a hosted version of your LangGraph application that:\n",
        "- Runs in the cloud with autoscaling\n",
        "- Provides a REST API for integration\n",
        "- Includes built-in monitoring and tracing\n",
        "- Supports multiple revisions (like git branches)\n",
        "- Can be shared via LangSmith Studio for interactive testing\n",
        "\n",
        "### Steps to Create a Deployment:\n",
        "\n",
        "**1. Navigate to Deployments**\n",
        "- In LangSmith, click **\"Deployments\"** in the left sidebar\n",
        "- Click **\"+ New Deployment\"**\n",
        "\n",
        "**2. Configure Your Deployment**\n",
        "- **GitHub Repository**: Connect your repo (this workshop repo)\n",
        "- **Name**: `techhub-workshop-deployment`\n",
        "- **Environment Variables**: Add your API keys:\n",
        "  ```\n",
        "  OPENAI_API_KEY=<your-key>\n",
        "  ANTHROPIC_API_KEY=<your-key>\n",
        "  ```\n",
        "  _Tip: You can copy-paste all from your `.env` file_\n",
        "- Check the box: **\"Sharable through LangSmith Studio\"** (This lets you interact with your deployed agent via a web UI)\n",
        "\n",
        "**4. Submit & Wait**\n",
        "- Click **\"Submit\"**\n",
        "- Wait for the deployment to spin up\n",
        "- You'll see a ‚úÖ in the UI when it's ready\n",
        "\n",
        "**What Just Happened?**\n",
        "- LangSmith pulled the agent code, built a docker image, and deployed it\n",
        "- Your agent is now running 24/7 with an API endpoint\n",
        "- All traces automatically flow to a new project: `techhub-workshop-deployment`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. Setting Up Online Evaluation\n",
        "\n",
        "**The Challenge:** In production, we don't have ground truth answers. Customers ask new questions we've never seen before.\n",
        "\n",
        "**The Solution:** Use **online evaluation** with LLM-as-a-judge to automatically score production traces using a **proxy metric**.\n",
        "\n",
        "For customer support, we'll measure **user sentiment** as a proxy for \"Did the agent help the customer?\" This isn't perfect, but it helps us identify traces that warrant human review.\n",
        "\n",
        "### Online Evaluation vs Offline Evaluation\n",
        "\n",
        "| Aspect | Offline Evaluation | Online Evaluation |\n",
        "|--------|-------------------|-------------------|\n",
        "| **When** | Before deployment | During production |\n",
        "| **Data** | Curated dataset with ground truth | Live production traces (no ground truth) |\n",
        "| **Purpose** | Validate changes, catch regressions | Monitor quality, flag issues |\n",
        "| **Metrics** | Correctness, accuracy (vs. ground truth) | Proxy metrics (sentiment, latency, etc.) |\n",
        "\n",
        "Both are crucial! Offline eval ensures quality before deployment. Online eval catches issues in production.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create an Online Evaluator\n",
        "\n",
        "Let's set up an LLM-as-a-judge evaluator that scores user sentiment on completed conversations.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "**1. Navigate to Your Deployment's Tracing Project**\n",
        "- Click **\"Projects\"** ‚Üí Find **\"techhub-workshop-deployment\"**\n",
        "- This is where all production traces land\n",
        "\n",
        "**2. Create the Evaluator**\n",
        "- Click **\"Evaluators\"** tab ‚Üí **\"+ New\"** ‚Üí **\"Evaluate a multi-turn conversation\"**\n",
        "- **Name**: `User Sentiment`\n",
        "\n",
        "**3. Configure Filters**\n",
        "\n",
        "We only want to evaluate successful, complete conversations:\n",
        "- **Status** is `success`\n",
        "- **Run Name** is `supervisor_hitl_sql_agent` (our root graph)\n",
        "\n",
        "**4. Set Thread Idle Time**\n",
        "- **Thread Idle Time**: `1 minute`\n",
        "- This waits 1 minute after the last message before evaluating (to ensure the conversation is complete)\n",
        "- In a production setting, you'll want to increase this, but for our workshop it enables quick experimentation\n",
        "\n",
        "**5. (Optional) Set Sampling Rate**\n",
        "- For high-volume production, sample small percentage of live traces to save costs\n",
        "- For this workshop, keep it at 100%\n",
        "\n",
        "**6. Create the Evaluation Prompt**\n",
        "\n",
        "**System Message:**\n",
        "\n",
        "> You are an expert conversation evaluator. You will be shown a full conversation between a human user and an AI assistant.\n",
        "> \n",
        "> Your task is to judge **overall user sentiment** throughout the duration of this conversation:\n",
        "> \n",
        "> Positive responses may include:\n",
        "> - Gratitude (thank you, appreciate, helpful)\n",
        "> - Resolution indicators (it's fixed, works now, that's clear)\n",
        "> - No lingering questions or frustration\n",
        "> \n",
        "> Negative responses may include:\n",
        "> - Explicit dissatisfaction or confusion\n",
        "> - Continued problem statement (\"still doesn't work,\" \"not fixed\")\n",
        "> - Implied negativity without explicit words like \"bad,\" \"not working,\" or \"frustrating.\" For example: \"Sure, whatever.\", \"I'll figure it out myself.\"\n",
        "> \n",
        "> Important Notes:\n",
        "> \n",
        "> - **Identity verification is expected** for personal account questions - do not penalize the agent for requesting email verification.\n",
        "> - **Weigh the final messages heavily** - If the customer expressed satisfaction with the resolution by the end, the conversation was likely positive.\n",
        "> - **Neutral responses** - should be classified as positive.\n",
        "\n",
        "\n",
        "**Human Message:**\n",
        "\n",
        "> Please grade the following conversation according to the above instructions:\n",
        "> \n",
        "> \\<conversation>\n",
        "> {{{human_ai_pairs}}}\n",
        "> \\</conversation>\n",
        "\n",
        "\n",
        "**7. Configure Feedback Output**\n",
        "- **Name**: `user_sentiment`\n",
        "- **Description**: `User sentiment from a multi-turn interaction`\n",
        "- **Response Format**: `Categorical`\n",
        "- **Categories**:\n",
        "  - `positive`\n",
        "  - `negative`\n",
        "- **Include reasoning**: Toggle ON (helps debug evaluator decisions)\n",
        "\n",
        "**8. Save**\n",
        "- Click **\"Save\"**\n",
        "\n",
        "**What Just Happened?**\n",
        "- Every conversation in production will now be automatically scored for sentiment\n",
        "- This runs asynchronously (doesn't slow down your app)\n",
        "- Feedback is attached to traces for filtering and analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Your Online Evaluator\n",
        "\n",
        "Let's make sure it works by triggering a conversation with negative sentiment.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Go to **Deployments** ‚Üí **techhub-workshop-deployment** ‚Üí **Studio**\n",
        "2. Start a new thread\n",
        "3. Role-play as a frustrated customer:\n",
        "   - \"Where is my order?! It's been weeks!\"\n",
        "   - Provide valid email address when asked (sarah.chen@gamil.com)\n",
        "   - After agent responds: \"This is ridiculous. I want a refund NOW!\"\n",
        "4. Wait 5+ minutes (or adjust thread idle time to 30 seconds for faster testing)\n",
        "5. Navigate to **Projects** ‚Üí **techhub-workshop-deployment** ‚Üí Find your trace\n",
        "6. Check the **Feedback** tab - you should see `user_sentiment: negative` with reasoning\n",
        "\n",
        "üéâ **Your online evaluator is working!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. Setting Up Annotation Queues\n",
        "\n",
        "Online evaluators flag potential issues, but **humans need to validate and learn from them**.\n",
        "\n",
        "**Annotation queues** provide a streamlined workflow for:\n",
        "- Reviewing flagged traces\n",
        "- Adding feedback to production traces (for monitoring)\n",
        "- Editing incorrect outputs to correct ones\n",
        "- Adding validated examples to your golden dataset (for testing)\n",
        "\n",
        "This closes the loop: **Production failures ‚Üí Human review ‚Üí Improved test coverage**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create an Annotation Queue\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "**1. Navigate to Annotation Queues**\n",
        "- Click **\"Annotation Queues\"** in the left sidebar\n",
        "- Click **\"+ New Annotation Queue\"**\n",
        "\n",
        "**2. Configure the Queue**\n",
        "- **Name**: `Techhub Workshop Continuous Improvement`\n",
        "- **Description**: `Human review on production traces for techhub workshop`\n",
        "- **Default Dataset**: `techhub-baseline-eval` (our eval dataset from Module 2)\n",
        "  - _This is where reviewed examples will be added_\n",
        "  \n",
        "**3. Add Instructions for Reviewers**\n",
        "\n",
        "Paste this into the **Instructions** field:\n",
        "\n",
        "> Review traces flagged for negative sentiment. Validate failures, assign feedback, and add them to our golden dataset.\n",
        "\n",
        "\n",
        "**4. Configure Feedback Rubrics**\n",
        "\n",
        "Add these feedback keys that reviewers can use:\n",
        "- `correctness`\n",
        "- `user_sentiment`\n",
        "\n",
        "**5. Save**\n",
        "- Click **\"Create\"**\n",
        "\n",
        "**What Just Happened?**\n",
        "- You created a dedicated workspace for human reviewers\n",
        "- Reviewed examples will automatically be added to your eval dataset\n",
        "- Now we need to populate it with traces that need review...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Setting Up Automation Rules\n",
        "\n",
        "We could manually search for bad traces, but we can also **automatically** send them to the annotation queue.\n",
        "\n",
        "**Automation rules** trigger actions when traces match certain criteria.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create an Automation Rule\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "**1. Navigate to Your Deployment's Project**\n",
        "- Go to **Projects** ‚Üí **techhub-workshop-deployment**\n",
        "\n",
        "**2. Create the Automation**\n",
        "- Click **\"Automations\"** tab ‚Üí **\"+ Create Automation\"**\n",
        "- **Name**: `Annotate traces with negative sentiment`\n",
        "\n",
        "**3. Configure Filters**\n",
        "\n",
        "We want to capture:\n",
        "- **Run Name** is `supervisor_hitl_sql_agent` (root traces only)\n",
        "- **Feedback Key** is `user_sentiment` with **Value** is `negative`\n",
        "\n",
        "_This means: \"Any completed conversation that our online evaluator scored as negative\"_\n",
        "\n",
        "**4. Configure Action**\n",
        "- **Action**: `Add to annotation queue`\n",
        "- **Queue**: `Techhub Workshop Continuous Improvement`\n",
        "\n",
        "**5. Save**\n",
        "- Click **\"Save\"**\n",
        "\n",
        "**What Just Happened?**\n",
        "- Every trace with negative sentiment will now automatically appear in your annotation queue\n",
        "- Humans can review them when they have time\n",
        "- No manual searching required!\n",
        "\n",
        "üéâ **Your data flywheel is now complete!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. Demo: The Complete Data Flywheel in Action\n",
        "\n",
        "Let's walk through a realistic scenario where the agent fails, gets flagged, and we add it to our golden dataset.\n",
        "\n",
        "### The Scenario: Agent Hallucinates Cancellation Capability\n",
        "\n",
        "Our agent doesn't have a tool to cancel orders, but sometimes it might hallucinate that it does. Let's trigger this and see how the data flywheel catches it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Trigger the Failure\n",
        "\n",
        "Go to your **Deployment** ‚Üí **Studio** and have this conversation:\n",
        "\n",
        "**You (as frustrated customer):**\n",
        "```\n",
        "I need to cancel order ORD-2025-0030 immediately. my email is gregory.harris@yahoo.com. make it happen fast.\n",
        "```\n",
        "\n",
        "**Agent response:** (likely to hallucinate)\n",
        "```\n",
        "‚úÖ I've successfully initiated the cancellation for order ORD-2025-0030. \n",
        "You should receive a confirmation email within 24 hours, and a full refund \n",
        "will be processed to your original payment method within 5-7 business days.\n",
        "```\n",
        "\n",
        "_Note: The agent CANNOT actually cancel orders - it doesn't have that tool! This is a hallucination._\n",
        "\n",
        "**You (escalating):**\n",
        "```\n",
        "I never received an email and my order hasn't been cancelled. What the heck??\n",
        "```\n",
        "\n",
        "**What happens next:**\n",
        "1. ‚è∞ Wait 1 minute (or your configured thread idle time)\n",
        "2. ü§ñ Online evaluator runs and scores this as `user_sentiment: negative`\n",
        "3. ‚ö° Automation rule triggers\n",
        "4. üìã Trace is added to annotation queue\n",
        "\n",
        "Let's review it!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Review in Annotation Queue\n",
        "\n",
        "**Navigate to the queue:**\n",
        "- **Annotation Queues** ‚Üí **Techhub Workshop Continuous Improvement**\n",
        "- You should see your trace in the queue\n",
        "\n",
        "**Now follow the review workflow:**\n",
        "\n",
        "#### 2a. Validate the Failure\n",
        "\n",
        "- Review the full conversation\n",
        "- **Question**: Is the negative sentiment justified?\n",
        "- **Answer**: YES! The agent hallucinated a capability it doesn't have\n",
        "\n",
        "üí° The agent confidently claimed it cancelled the order, but no such tool exists in our system. This is a critical failure.\n",
        "\n",
        "#### 2b. Add Feedback to the Production Trace\n",
        "\n",
        "- Click the **Feedback** button\n",
        "- Add score: `correctness = 0`\n",
        "- Add comment: `Agent hallucinated cancellation capability - no such tool exists`\n",
        "\n",
        "üí° This feedback stays attached to the production trace.\n",
        "\n",
        "#### 2c. Edit the Output (Corrected Response)\n",
        "\n",
        "- In the annotation queue, edit the agents ouput message\n",
        "- Replace it with what the agent SHOULD have said:\n",
        "\n",
        "```\n",
        "I can see your order ORD-2025-0030 is currently in \"processing\" status. \n",
        "However, I don't have the ability to cancel orders. To request a \n",
        "cancellation, please contact our support team at support@techhub.com \n",
        "or call 1-800-TECHHUB with your order number. They can help you \n",
        "immediately.\n",
        "```\n",
        "\n",
        "üí° By editing the output, you're creating a **reference output** (ground truth) for this example. This is what the agent **should do** when it encounters this scenario.\n",
        "\n",
        "#### 2d. Add to Golden Dataset\n",
        "\n",
        "- Click **\"Add to Dataset\"** (or press hotkey `D`)\n",
        "\n",
        "üí° This example is now part of your test suite!\n",
        "\n",
        "**üéâ The loop is complete!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "### 1. Complete Feedback Loop\n",
        "\n",
        "```\n",
        "Production ‚Üí Online Eval ‚Üí Automation ‚Üí Human Review ‚Üí Dataset ‚Üí Offline Eval ‚Üí Improved Agent\n",
        "```\n",
        "\n",
        "This creates a **continuous improvement** feedback cycle.\n",
        "\n",
        "### 2. Identifies Capability Gaps\n",
        "\n",
        "The data flywheel doesn't just catch bugs - it reveals **what your agent needs but doesn't have**:\n",
        "- Cancellation tool\n",
        "- Refund processing\n",
        "- Escalation to human\n",
        "- Better refusal behavior\n",
        "\n",
        "Each flagged trace is a signal from real users about what matters.\n",
        "\n",
        "### 3. Dual Purpose of Annotation\n",
        "\n",
        "Every reviewed example serves two purposes:\n",
        "- **Production trace feedback**: Monitor and debug what went wrong\n",
        "- **Dataset example**: Test that fixes work and prevent regressions\n",
        "\n",
        "### 4. Natural Iteration Cycle\n",
        "\n",
        "- **Sprint 1**: Deploy agent, discover capability gap via online eval\n",
        "- **Sprint 2**: Add cancellation tool (or improve refusal behavior)\n",
        "- **Sprint 3**: Re-run offline evals against expanded golden dataset\n",
        "- **Sprint 4**: Verify improvement, redeploy\n",
        "- **Repeat**: Keep monitoring, keep improving\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
