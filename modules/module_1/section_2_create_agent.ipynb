{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1, Section 2: Building Your First Agent\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "- Replace manual tool calling with the `create_agent` abstraction\n",
    "- Add persistent memory to enable multi-turn conversations\n",
    "- Understand threads for managing separate conversations\n",
    "- See streaming responses in action\n",
    "\n",
    "## Overview\n",
    "\n",
    "In Section 1, we built the tool calling loop manually. While educational, that's a lot of tedious code!\n",
    "\n",
    "In this section, we'll use `create_agent` - a powerful abstraction that:\n",
    "- Handles the entire tool calling loop automatically\n",
    "- Adds conversation memory out of the box\n",
    "- Supports streaming for better UX\n",
    "- Requires just a few lines of code\n",
    "\n",
    "By the end, you'll see how `create_agent` replaces ~50 lines of manual code with ~5 lines!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Load environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Tools\n",
    "\n",
    "**Note on Refactoring:** In Section 1, we defined tools inline for learning purposes. Now that you understand how tools work, we've moved them to the shared `tools/` directory for reuse across multiple sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tools imported from tools/database.py!\n",
      "  - get_order_status\n",
      "  - get_product_price\n"
     ]
    }
   ],
   "source": [
    "# Import our shared database tools\n",
    "from tools import get_order_status, get_product_price\n",
    "\n",
    "print(\"✓ Tools imported from tools/database.py!\")\n",
    "print(f\"  - {get_order_status.name}\")\n",
    "print(f\"  - {get_product_price.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Your First Agent\n",
    "\n",
    "This is where the magic happens! Let's replace all that manual loop code with the `create_agent` abstraction which will run the loop for us:\n",
    "1. Model decides which tool to call (if any)\n",
    "2. Tool gets executed\n",
    "3. Result goes back to model\n",
    "4. Repeat until task is complete\n",
    "\n",
    "The prebuilt agent handles running the loop described above - you just specify the system prompt and tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Configure model\n",
    "MODEL = \"anthropic:claude-haiku-4-5\"\n",
    "llm = init_chat_model(MODEL)\n",
    "\n",
    "# Create agent - THIS REPLACES ALL THE MANUAL LOOP CODE!\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[get_order_status, get_product_price],\n",
    "    system_prompt=\"You are a helpful customer support assistant for TechHub.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the benefits:\n",
    "  - Automatic tool calling loop\n",
    "  - No manual message management\n",
    "  - Just ~5 lines of code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it with the same query as Section 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What's the status of order ORD-2024-0123?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "[{'id': 'toolu_015SeiNz8w8WUR59qc3drmBL', 'input': {'order_id': 'ORD-2024-0123'}, 'name': 'get_order_status', 'type': 'tool_use'}]\n",
      "Tool Calls:\n",
      "  get_order_status (toolu_015SeiNz8w8WUR59qc3drmBL)\n",
      " Call ID: toolu_015SeiNz8w8WUR59qc3drmBL\n",
      "  Args:\n",
      "    order_id: ORD-2024-0123\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_order_status\n",
      "\n",
      "Order ORD-2024-0123: Status=Delivered, Shipped=2024-12-07, Tracking=1Z999AA113527782\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Great! Here's the status of your order:\n",
      "\n",
      "**Order ID:** ORD-2024-0123\n",
      "- **Status:** Delivered ✓\n",
      "- **Shipped Date:** December 7, 2024\n",
      "- **Tracking Number:** 1Z999AA113527782\n",
      "\n",
      "Your order has been successfully delivered! If you have any other questions about this order or need further assistance, feel free to let me know.\n"
     ]
    }
   ],
   "source": [
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What's the status of order ORD-2024-0123?\"}\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "for message in result[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice:** This works great for single queries! But try a follow-up question..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'd be happy to help you find out when your order was shipped! However, I need your order ID to look up that information.\n",
      "\n",
      "Could you please provide your order ID? It typically looks something like \"ORD-2024-0123\".\n"
     ]
    }
   ],
   "source": [
    "# Try a follow-up that references the previous query\n",
    "result = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"When was it shipped?\"}]}\n",
    ")\n",
    "\n",
    "result[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️  The agent doesn't remember the previous order! We need memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add Short-term Memory with Checkpointer\n",
    "\n",
    "Right now, each agent invocation is independent. Let's add **short-term memory** so the agent can maintain context across multi-turn conversations.\n",
    "\n",
    "LangGraph uses **checkpointers** to save and restore state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Add checkpointer for memory\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "agent_with_memory = create_agent(\n",
    "    model=llm,\n",
    "    tools=[get_order_status, get_product_price],\n",
    "    system_prompt=\"You are a helpful customer support assistant for TechHub.\",\n",
    "    checkpointer=checkpointer,  # This enables memory!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we also need to pass a `thread_id` when invoking the agent. The `thread_id` acts as a unique identifier for each conversation, allowing the agent to keep separate histories for different users or sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Turn 1]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your order **ORD-2024-0123** has been **delivered**! \n",
      "\n",
      "Here are the details:\n",
      "- **Status:** Delivered\n",
      "- **Shipped Date:** December 7, 2024\n",
      "- **Tracking Number:** 1Z999AA113527782\n",
      "\n",
      "If you have any other questions about your order, feel free to ask!\n",
      "\n",
      "[Turn 2]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "According to the order status, your order **ORD-2024-0123** was shipped on **December 7, 2024**.\n",
      "\n",
      "✓ The agent remembers we're talking about ORD-2024-0123!\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "# Create a thread for this conversation - common practice to use a uuid\n",
    "thread_id = str(uuid.uuid4())\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "# Turn 1: Ask about an order\n",
    "print(\"[Turn 1]\")\n",
    "result = agent_with_memory.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What's the status of order ORD-2024-0123?\"}\n",
    "        ]\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "result[\"messages\"][-1].pretty_print()\n",
    "\n",
    "# Turn 2: Follow-up question (references \"it\" from previous turn)\n",
    "print(\"\\n[Turn 2]\")\n",
    "result = agent_with_memory.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"When was it shipped?\"}]},\n",
    "    config=config,  # same thread_id\n",
    ")\n",
    "result[\"messages\"][-1].pretty_print()\n",
    "\n",
    "print(\"\\n✓ The agent remembers we're talking about ORD-2024-0123!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Thread Separation\n",
    "\n",
    "Different `thread_id`s create separate conversations with isolated memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Customer A]: Great! Here's the status of your order:\n",
      "\n",
      "**Order ID:** ORD-2024-0123\n",
      "- **Status:...\n",
      "\n",
      "[Customer B]: The **MacBook Air M2 (13-inch, 256GB)** is priced at **$1,199.00** and is curren...\n",
      "\n",
      "[Customer A follow-up]: According to the order status I just looked up, your order **ORD-2024-0123 was s...\n",
      "\n",
      "✓ Thread 1 remembers order ORD-2024-0123, Thread 2 has no knowledge of it!\n"
     ]
    }
   ],
   "source": [
    "# Thread 1: Customer A\n",
    "config_user1 = {\"configurable\": {\"thread_id\": \"customer-A\"}}\n",
    "result = agent_with_memory.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What's the status of order ORD-2024-0123?\"}\n",
    "        ]\n",
    "    },\n",
    "    config=config_user1,\n",
    ")\n",
    "print(f\"[Customer A]: {result['messages'][-1].content[:80]}...\\n\")\n",
    "\n",
    "# Thread 2: Customer B (different conversation)\n",
    "config_user2 = {\"configurable\": {\"thread_id\": \"customer-B\"}}\n",
    "result = agent_with_memory.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"How much is the MacBook Air?\"}]},\n",
    "    config=config_user2,\n",
    ")\n",
    "print(f\"[Customer B]: {result['messages'][-1].content[:80]}...\\n\")\n",
    "\n",
    "# Back to Thread 1: Memory is preserved\n",
    "result = agent_with_memory.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"When was it shipped?\"}]},\n",
    "    config=config_user1,\n",
    ")\n",
    "print(f\"[Customer A follow-up]: {result['messages'][-1].content[:80]}...\")\n",
    "\n",
    "print(\"\\n✓ Thread 1 remembers order ORD-2024-0123, Thread 2 has no knowledge of it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway:\n",
    "- Checkpointers enable memory across interactions\n",
    "- Thread IDs separate different conversations\n",
    "- State persists automatically - no manual state management needed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Streaming Responses for Better UX\n",
    "\n",
    "LLMs can take a while to respond.\n",
    "\n",
    "**Streaming** shows progress in real-time, dramatically improving user experience. To stream responses token-by-token - just change `.invoke()` to `.stream()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response:\n",
      "\n",
      "Great! Here's the status of your order:\n",
      "\n",
      "**Order ORD-2024-0125**\n",
      "- **Status:** Delivered ✓\n",
      "- **Shipped Date:** December 14, 2024\n",
      "- **Tracking Number:** 1Z999AA127417599\n",
      "\n",
      "Your order has been successfully delivered! If you have any questions about your delivery or need further assistance, feel free to ask."
     ]
    }
   ],
   "source": [
    "print(\"Streaming response:\\n\")\n",
    "\n",
    "for message_chunk, metadata in agent.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What's the status of order ORD-2024-0125?\"}\n",
    "        ]\n",
    "    },\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if metadata.get(\"langgraph_node\") == \"model\":  # only print the model's output\n",
    "        for block in message_chunk.content_blocks:\n",
    "            if block.get(\"type\") == \"text\" and block.get(\"text\"):\n",
    "                print(block.get(\"text\"), end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** You can control what types of information are streamed (e.g., messages, tool calls, etc.) by setting the `stream_mode` parameter. This allows fine-grained control over what you receive in real-time. See the [LangGraph streaming docs](https://docs.langchain.com/oss/python/langgraph/streaming) for details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **`create_agent` is powerful** - Replaces ~50 lines of manual code with ~5 lines\n",
    "2. **Checkpointers enable memory** - Add `checkpointer=MemorySaver()` for conversation memory\n",
    "3. **Threads separate conversations** - Use unique `thread_id` for each user/conversation\n",
    "4. **Streaming improves UX** - Just change `.invoke()` to `.stream()`\n",
    "\n",
    "### What's Next\n",
    "\n",
    "In **Section 3**, we'll build a **multi-agent system**:\n",
    "- Specialized sub-agents (Database Agent, RAG Agent)\n",
    "- Supervisor pattern for routing\n",
    "- Coordinating complex workflows\n",
    "\n",
    "The simple agent you built here becomes the building block for more sophisticated systems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
